
\section{Recursive neural network models} \label{methods}

We study neural models that adhere to the linguistic \ii{principle of
 compositionality}, which says that the meanings for complex
expressions are derived from the meanings of their constituent parts
via specific composition functions \cite{Partee84,Janssen97}. In our
distributed setting, word meanings are vectors of length $n$. The
composition function maps pairs of them to single vectors of length $n$, 
which can then be merged again to represent more complex
phrases. Once the entire sentence-level representation has been
derived, it serves as a fixed-length input for some subsequent function.

We use the model architecture depicted in Figure~\ref{sample-figure}. 
The two phrases being compared are processed separately using a pair 
of recursive networks that share a single set of parameters. 
The resulting vectors are fed into a separate comparison
layer that is meant to generate a feature vector capturing the
relation between the two phrases. The output of this layer is then
given to a softmax classifier, which in turn produces a hypothesized
distribution over the seven relations represented in Table~\ref{b-table}.

For a composition layer, we evaluate models with both the plain neural
network layer function \eqref{rnn} and a more powerful extension, the RNTN layer function
\eqref{rntn} proposed in \newcite{chen2013learning}. A $\tanh$ nonlinearity is applied to the output of the layer function.
%
\begin{gather} \label{rnn}
\vec{y}_{\textit{RNN}} = f(\mathbf{M} [\vec{x}^{(l)}; \vec{x}^{(r)}] + \vec{b}) \\ % TODO: Add column vectors?
\label{rntn}
\vec{y}_{\textit{RNTN}} = \vec{y}_{\textit{RNN}} + f(\vec{x}^{(l)T} \mathbf{T}^{[1 \ldots n]} \vec{x}^{(r)})
\end{gather} 
%
Here, $\vec{x}^{(l)}$ and $\vec{x}^{(r)}$ are the column vector
representations for the left and right children of the node, and
$\vec{y}$ is the node's output.  The RNN concatenates them, multiplies
them by an $n \times 2n$ matrix of learned weights, adds a bias $\vec{b}$, and applies the
element-wise non-linearity to the resulting vector. The RNTN adds a learned third-order tensor $\mathbf{T}$, dimension $n \times n \times n$, modeling
multiplicative interactions between the child vectors.

% TODO: Kill ReLU?

The comparison layer uses the same layer function as the
composition layers (either an NN layer or an NTN layer) with
independently learned parameters and a separate nonlinearity function.
Rather than use a $\tanh$ nonlinearity here, we use a rectified linear function. In
particular, we use the leaky rectified linear function
\shortcite{maasrectifier}: $f(\vec{x})=\max(\vec{x}, 0) +
0.01\min(\vec{x}, 0)$, applied element-wise.

To run the model forward and label a pair of phrases, the structure of
the lower layers of the network is assembled so as to mirror the tree
structures provided for each phrase. The word vectors are then looked
up from the vocabulary matrix $V$ (one of the model parameters), and
the composition and comparison functions are used to pass information
up the tree and into the classifier at the top. For an objective
function, we use the negative log of the probability assigned to the
correct label.

% Removed 'minibatch' -> For two of the experiments, we use minibatches
% of size 1, which doesn't really count.
We train the model using stochastic gradient descent (SGD)
with learning rates computed using AdaGrad \cite{duchi2011adaptive}.
The parameters (including the vocabulary) are initialized randomly.
Because the classes are not balanced in general, we report performance
using both accuracy and macroaveraged F1, since the latter emphasizes
 performance on infrequent classes. We compute macroaveraged F1 
as the harmonic mean of mean precision and mean recall, both computed
for all classes for which there is test data, setting precision to 0 
where it is not defined. Source code and generated data will be released
after the review period.

% TODO: Describe mean-of-five scheme for results reporting.

\begin{figure*}[tp]
  \centering
 % \includegraphics[scale=0.35]{model.eps}
  \input{figure1}
  \caption{The model structure used to compare \ii{((all reptiles) walk)} and \ii{((some turtles) move)}. 
    The same structure is used for both the RNN and RNTN layer functions.} 
  \label{sample-figure}
\end{figure*}
