
\section{Recursive neural network models} \label{methods}

We study neural models that adhere to the linguistic \ii{principle of
 compositionality}, which says that the meanings for complex
expressions are derived from the meanings of their parts
via specific composition functions \cite{Partee84,Janssen97}. In our
distributed setting, word meanings are vectors of dimension $n$. The
composition function maps pairs of them to single vectors of dimension $n$, 
which can then be merged again to represent more complex
phrases. Once the entire sentence-level representation has been
derived, it serves as a fixed-dimensional input for some subsequent function.

To apply recursive models to this inference class, we propose the tree 
pair model architecture depicted in Figure~\ref{sample-figure}, which is novel but
closely modeled after past work on recursive NNs. 
In it, the two phrases being compared are processed separately using a pair 
of recursive networks that share a single set of parameters. 
The resulting vectors are fed into a separate comparison
layer that is meant to generate a feature vector (fixed at 75 dimensions) capturing the
relation between the two phrases. The output of this layer is then
given to a softmax classifier, which in turn produces a hypothesized
distribution over the seven relations represented in Table~\ref{b-table}.

For a composition layer, we evaluate models with both the plain neural
network layer function \eqref{rnn} and the more powerful RNTN layer function
\eqref{rntn} proposed in \newcite{chen2013learning}. The nonlinearity $f(x) = \tanh(x)$ is applied to the output of the layer function.
%
\begin{gather} \label{rnn}
\vec{y}_{\textit{RNN}} = f(\mathbf{M} [\vec{x}^{(l)}; \vec{x}^{(r)}] + \vec{b}\,) \\
\label{rntn}
\vec{y}_{\textit{RNTN}} = \vec{y}_{\textit{RNN}} + f(\vec{x}^{(l)T} \mathbf{T}^{[1 \ldots n]} \vec{x}^{(r)})
\end{gather} 
%
Here, $\vec{x}^{(l)}$ and $\vec{x}^{(r)}$ are the column vector
representations for the left and right children of the node, and
$\vec{y}$ is the node's output.  The RNN concatenates them, multiplies
them by an $n \times 2n$ matrix of learned weights, adds a bias $\vec{b}$, and applies the
non-linearity element-wise to the resulting vector. The RNTN adds a learned third-order tensor 
$\mathbf{T}$, of dimension $n \times n \times n$, modeling
multiplicative interactions between the child vectors.

The comparison layer uses the same layer function as the
composition layers (either an NN layer or an NTN layer) with
independently learned parameters and a separate nonlinearity function.
Rather than use a $\tanh$ nonlinearity here, we found better results with a rectified linear function. In
particular, we use the leaky rectified linear function
\cite{maasrectifier}: $f(x)=\max(x, 0) +
0.01\min(x, 0)$.

To run the model forward and label a pair of phrases, the structure of
the lower layers of the network is assembled so as to mirror the tree
structures provided for each phrase. The word vectors are then looked
up from the vocabulary matrix $V$ (one of the model parameters), and
the composition and comparison functions are used to pass information
up the tree and into the classifier at the top. For an objective
function, we use the negative log of the probability assigned to the
correct label, plus L2 regularization.

% Removed 'minibatch' -> For two of the experiments, we use minibatches
% of size 1, which doesn't really count.
We train the model using stochastic gradient descent (SGD)
with learning rates computed using AdaDelta \cite{zeiler2012adadelta}.
Training times on CPUs vary from hours to days across experiments, and
the hyperparameters of AdaDelta, the dimensions of the representations,
and the initialization strategy are all tuned manually.
Because the classes are not balanced in the artificial data experiments, we report performance
using both accuracy and macroaveraged F1, since the latter emphasizes
 performance on infrequent classes. We compute macroaveraged F1 
as the harmonic mean of average precision and average recall, both computed
for all classes for which there is test data, setting precision to 0 
where it is not defined. On the artificial data experiments, we report mean
results over five fold cross-validation, though differences across runs are small.
Source code and generated data will be released after the review period.

\begin{figure}[tp]
  \centering
 % \includegraphics[scale=0.35]{model.eps}
  \input{figure1}
  \caption{The model structure used to compare \ii{((all reptiles) walk)} and \ii{((some turtles) move)}. 
    The same structure is used for both the RNN and RNTN layer functions.} 
  \label{sample-figure}
\end{figure}
