\section{Reasoning about semantic relations}\label{sec:join}

If a model is to learn the behavior of a relational logic like the one
presented here from a finite amount of data, it must learn to deduce new
relations from seen relations in a sound manner. The simplest such
deductions involve atomic statements using the relations in
Table~\ref{b-table}. For instance, given that $a \natrev b$ and $b
\natrev c$, one can conclude that $a \natrev c$, by basic
set-theoretic reasoning (transitivity of $\natrev$). Similarly, from
$a \natfor b$ and $b \natneg c$, it follows that $a \natalt c$.  The
full set of sound inferences of this form is summarized in
Table~\ref{tab:jointable}.

% about the relations themselves that do not depend on the
% internal structure of the things being compared. For example, given
% that $a\sqsupset b$ and $b\sqsupset c$ one can conclude that
% $a\sqsupset c$ by the transitivity of $\sqsupset$, even without
% understanding $a$, $b$, or $c$. These seven relations support more
% than just transitivity: MacCartney and Manning's
% \cite{maccartney2009extended} join table defines 32 valid inferences
% that can be made on the basis of pairs of relations of the form $a R
% b$ and $b R' c$, including several less intuitive ones such as that if
% $a \natneg b$ and $b~|~c$ then $a \sqsupset c$.

\begin{table}[htp]
  \centering  \small
  \setlength{\arraycolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\UNK}{\cdot}  
  $\begin{array}[t]{c@{ \ }|*{7}{c}|}
    %\hline
    \multicolumn{1}{c}{}
             & \nateq     & \natfor     & \natrev     & \natneg    & \natalt     & \natcov     & \multicolumn{1}{c}{\natind} \\
    \cline{2-8}
    \nateq  & \nateq &   \natfor &  \natrev &  \natneg &   \natalt &  \natcov &  \natind \\
    \natfor & \natfor &  \natfor &  \UNK &  \natalt &   \natalt &  \UNK &  \UNK \\
    \natrev & \natrev &  \UNK &  \natrev &  \natcov &   \UNK &  \natcov &  \UNK \\
    \natneg & \natneg &  \natcov &  \natalt &  \nateq &    \natrev &  \natfor &  \natind \\
    \natalt & \natalt &  \UNK &  \natalt &  \natfor &   \UNK &  \natfor &  \UNK \\
    \natcov & \natcov &  \natcov &  \UNK &  \natrev &   \natrev &  \UNK &  \UNK \\
    \natind & \natind & \UNK &  \UNK &  \natind &  \UNK &  \UNK &  \UNK \\
    \cline{2-8}
  \end{array}$
  \caption{Inference path from premises $a\,R\,b$ (row) and $b\,S\,c$ (column) to the relation that holds between $a$ and $c$, if any.  These inferences are based on basic set-theoretic truths about the meanings of the underlying relations as described in Table~\ref{b-table}. We assess our models' ability to reproduce such inferential paths. Cells containing a dot correspond to pairs
of relations for which no valid inference can be drawn.}
  \label{tab:jointable}
\end{table}

\paragraph{Experiments}
To test our models' ability to learn these inferential patterns, we
create small boolean structures for our logic in which terms denote
sets of entities from a small domain.  Figure~\ref{lattice-figure}
depicts a structure of this form. The lattice gives the full model,
for which all the statements on below are valid. We divide these
statements evenly into training and test sets, and remove from the
test set statements which cannot be proven from the training
statements.
% using the logic depicted in Figure~\ref{lattice-figure}.

In each experimental run, we create 80 randomly generated sets drawing from
a domain of seven elements. This yields a data set consisting of
6400 statements about pairs of formulae. 3200 of these pairs are
chosen as a test set, and that test set is further reduced to the 2960
examples that can be provably derived from the training data.

We trained versions of both the RNN model and the RNTN model on these
data sets. In both cases, the models are implemented exactly as
described in Section~\ref{methods}, but since the items being compared
are single terms rather than full tree structures, the composition
layer is not involved, and the two models differ only in which
layer function is used for the comparison layer. We simply present
the models with two single terms, each of which corresponds to a
single vector in the (randomly initialized) vocabulary matrix $V$,
thereby ensuring that the model has no information about the terms
being compared except the relations between them.


\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \newcommand{\labelednode}[4]{\put(#1,#2){\oval(1.5,1)}\put(#1,#2){\makebox(0,0){$\begin{array}{c}#3\\\{#4\}\end{array}$}}}
    \setlength{\unitlength}{1cm}\scalebox{0.7}{
    \begin{picture}(5,5.5)
      \labelednode{2.50}{5}{}{1,2,3}
      
      \put(0.75,4){\line(3,1){1.5}}
      \put(2.5,4){\line(0,1){0.5}}
      \put(4.25,4){\line(-3,1){1.5}}
      
      \labelednode{0.75}{3.5}{a,b}{1,2}
      \labelednode{2.50}{3.5}{c}{1,3}
      \labelednode{4.25}{3.5}{d}{2,3}
      
      \put(0.75,2.5){\line(0,1){0.5}}
      \put(0.75,2.5){\line(3,1){1.5}}
      
      \put(2.5,2.5){\line(-3,1){1.5}}
      \put(2.5,2.5){\line(3,1){1.5}}
      
      \put(4.25,2.5){\line(0,1){0.5}}
      \put(4.25,2.5){\line(-3,1){1.5}}
      

      \labelednode{0.75}{2}{e,f}{1}
      \labelednode{2.50}{2}{}{2}
      \labelednode{4.25}{2}{g,h}{3}
      
      \put(2.5,1){\line(-3,1){1.5}}
      \put(2.5,1){\line(0,1){0.5}}
      \put(2.5,1){\line(3,1){1.5}}
      
      \labelednode{2.5}{0.5}{}{}
    \end{picture}}
    \caption{Simple boolean structure. The letters name the sets. Not all sets have names, and
    some sets have multiple names, so that learning $\nateq$ is non-trivial.}
  \end{subfigure}
  \qquad
    \begin{subfigure}[t]{0.43\textwidth}
    \centering \vspace{0.4cm}
    \setlength{\tabcolsep}{12pt}
    \begin{tabular}[b]{c  c}
      \toprule
      Train & Test \\
      \midrule

      $a \nateq b$              & $b \nateq b$ \\
      $a \natfor e$              & $b \natfor e$ \\
      $a \natneg g$              & \strikeout{$e \nateq f$} \\
      $d \natfor h$              & \strikeout{$g \natrev d$} \\
      $e \natalt g$            & $h \natrev d$ \\

      \bottomrule
    \end{tabular}

    \caption{A few examples of atomic statements about the
      model.  Test statements that are not provable from the training data shown are
      crossed out.}
  \end{subfigure}  
  \caption{Small example structure and data for learning relation composition.}
  \label{lattice-figure}
\end{figure} 


\paragraph{Results} 
Table \ref{joinresultstable} shows the results, which are straightforward 
to interpret. The RNTN is able to accurately encode the relations 
between the terms in the geometric relations between their vectors, 
and is able to then use that information to recover relations that 
are not overtly included in the training data. The RNN, in contrast, was not able to learn
to do this well under these conditions, though we cannot rule
out the possibility that different optimization techniques or further hyperparameter 
tuning could lead an RNN model to succeed here.


% Note: None of these test examples is derivable from the shown training data, 
% but we suggest that there is additional training data, so we can cross lines
% out willy-nilly without fear.
\begin{table}[tp]
  \centering \small
  \begin{tabular}{ l r@{ \ }r r@{ \ }r r@{ \ }r }
    \toprule
    ~&\multicolumn{2}{c}{$\natind$ only} & \multicolumn{2}{c}{15d RNN}  & \multicolumn{2}{c}{15d RNTN}\\
    \midrule
    Train &53.8 &(10.5)    & 99.8&	(99.0) & \textbf{100} & \textbf{(100)}\\
    Test &52.0 &(10.3) &	94.0&(87.0)& \textbf{99.6} & \textbf{(95.5)}\\
    \bottomrule
  \end{tabular}
  \caption{Performance on the semantic relation experiments. These results and all other results on artificial data are reported as mean accuracy scores over five runs followed by mean macroaveraged F1 scores in parentheses.}
  \label{joinresultstable}
\end{table}

