\begin{abstract}
  Natural logic offers a powerful relational conception of meaning
  that is a natural counterpart to distributed semantic
  representations, which have proven valuable in a wide range of
  sophisticated language tasks. However, it remains an open question
  whether it is possible to train distributed representations to
  support the rich, diverse logical reasoning captured by natural
  logic. We address this question using two neural network-based
  models for learning embeddings: plain neural networks and neural
  tensor networks. Our experiments evaluate the models' ability to
  learn the basic algebra of natural logic relations from simulated
  data and from the WordNet noun graph.  The overall positive results
  are promising for the future of learned distributed representations in
  the applied modeling of logical semantics.
\end{abstract}


